# CoDeR-RL: Reward-Shaped Reinforcement Learning for Efficient Reasoning in LLMs
Lenovo â€” Summer 2025  
**Base Model:** Qwen2.5-3B-Instruct  
---

## ğŸ§© Overview
**CoDeR-RL** (Cosine + Delta + Regularization Reinforcement Learning) is a reward-shaping framework that improves reasoning quality and stability in small-to-mid-sized large language models.  
It builds on NVIDIAâ€™s Tool-N1 baseline but replaces the brittle **binary reward** with a structured, multi-component design that teaches *how* to reason betterâ€”not just *whether* the model is right.

---

## âš™ï¸ Reward Design

### 1ï¸âƒ£ Cosine Reward
Smoothly scales reward by **answer correctness** and **reasoning length**.  
- Higher reward for short, correct answers.  
- Soft penalty for long or incorrect outputs.  
- Prevents runaway reasoning chains.

### 2ï¸âƒ£ Delta Reward
Rewards only **improvement between steps**, not cumulative reward.  
Formula: `r_k = Î± Ã— (r_k âˆ’ r_{k+1})`  
- Promotes genuine reasoning progress.  
- Eliminates â€œreward farmingâ€ from repetition.

### 3ï¸âƒ£ Clip Regularization
Caps per-step rewards at a threshold Î·.  
Formula: `r(q,p_k) = min(r_process(q,p_k) âˆ’ Î·, 0)`  
- Stops reward inflation from verbose steps.  
- Stabilizes gradients and limits over-optimization.

ğŸ§  **Combined Effect**  
Cosine governs *scaling*, Delta enforces *progress-based learning*, and Clip maintains *stability*.  
Together they yield cleaner reasoning, shorter chains of thought, and smoother reward curves.

---

### ğŸ§­ Reward Interaction Diagram
```text
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                 Reward Signal                â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Step-Level Reward Components   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼                  â–¼
   [ Cosine Reward ]   [ Delta Reward ]   [ Clip Regularizer ]   (othersâ€¦)
  scales by length &   rewards only step   caps per-step reward Î·
  correctness â†’ concise   improvements â†’     â†’ prevents loops &
  reasoning                genuine progress    verbosity
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼                  â–¼
            combine signals â†’ stabilize â†’ scale â†’ bound
                     â”‚
                     â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Total Shaped Reward rÌ‚    â”‚
           â”‚   (used by GRPO / PPO loop)â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
       improved reasoning efficiency:
         â€¢ shorter CoTs
         â€¢ higher accuracy
         â€¢ smoother training
